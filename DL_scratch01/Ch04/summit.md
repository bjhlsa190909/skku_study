4. 신경망 학습
--------------

4.1.1 데이터 주도 학습

<img width="400" height = '300' alt="fig 4-2" src="https://user-images.githubusercontent.com/125746059/229109804-2d041d40-0acb-49ec-bedb-d6db604c55cf.png">

기계학습의 경우 주어진 데이터에서 feature값을 추출하고 그 특징의 패턴을 기계가 학습하는 방법을 취하나, 신경망의 경우 데이터를 '있는 그대로' 학습하여 문제의 패턴을 찾아내려고 함. 

4.1.2 훈련 데이터와 시험 데이터

기계학습의 문제는 training data 와 test data로 나누어 학습과 실험을 수행하는게 일반적임(범용 능력을 제대로 평가하기 위함)

* 용어 설명 > 오버 피팅(overfitting) : 한 데이터 셋에만 지나치게 최적화되어 다른 데이터 셋에 대한 학습과 예측에는 신뢰도가 낮은 상태

4.2 손실 함수

손실함수는 신경망 성능의 '나쁨'을 나타내는 지표로, 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하느냐를 나타냄. 

4.2.1 오차제곱합

<img width="175" alt="e 4 1" src="https://user-images.githubusercontent.com/125746059/229112975-2e455697-7c38-4306-83cd-b3515d6ccaa2.png">

(yk는 신경망이 추정한 값, tk는 정답레이블, k는 데이터의 차원 수)

4.2.2 교차 엔트로피 오차

<img width="169" alt="e 4 2" src="https://user-images.githubusercontent.com/125746059/229115249-3c305647-15ab-477e-bfca-0fa1489a8ddc.png">

<img width="400" height='300' alt="fig 4-3" src="https://user-images.githubusercontent.com/125746059/229115824-ef7b69a6-d6d1-4fd3-99fa-089bf028e32c.png">
                         
교차 엔트로피 오차 또한 정답일 때의 출력(yk)이 전체 값을 결정하게 되고, 정답에 해당하는 yk값이 작을수록 오차는 커지게 됨. 

4.2.3 미니배치 학습

<img width="248" alt="e 4 3" src="https://user-images.githubusercontent.com/125746059/229117648-696bbd6f-2d58-4861-8521-6c05f26fc4be.png">

위 식은 전체 데이터 N개에 대한 평균 손실 함수를 구하는 식임. 

모든 데이터를 대상으로 손실함수의 합을 구하면 시간이 오래 걸리기에 이 경우 데이터 일부를 추려 전체의 '근사치'로 이용할 수 있는데, 이 중 일부를 '미니배치(mini-batch)'라고 함.
(예컨데 MNIST 데이터셋의 훈련데이터 6만장 중 100장만 무작위로 뽑아 학습함)

4.2.5 손실함수를 설정하는 이유

논의의 궁극적인 목적은 높은 '정확도'를 끌어내는 매개변수를 찾는 것임. 그럼에도 정확도라는 지표 대신 손실함수를 사용하는 이유는 '미분'의 역할에 있음. 

신경망 학습에서는 최적의 매개변수(편향과 가중치)를 탐색할 때 손실함수의 값을 가능한 한 가장 작게 하는 변수 값을 찾는 과정이고, 이 때 매개변수의 미분을 계산하고 그 값을 단서로 매개변수 값을 서서히 갱신하는 과정을 반복함.(정확도는 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없음)

4.4.1 경사(하강)법

일반적인 손실함수의 모습은 매우 복잡하고, 매개변수의 공간이 광대하여 어디가 최소값이 되는 곳인지 짐작하기 어려움. 이런 상황에서 기울기를 잘 이용하여 함수의 최소값(또한 가능한 한 가장 작은 값)을 찾으려는 것이 경사법임. 

<img width="153" alt="e 4 7" src="https://user-images.githubusercontent.com/125746059/229258852-f17d07bc-0539-49a4-8865-c0f94c11ead4.png">

(미분 값 앞에 붙어있는 기호는 eta라고 표현되며 갱신하는 양을 뜻하고 신경망 학습에서는 학습률learning rate로 표현됨)

ex) 예시 코드 구현
```python

def gradient_descent(f, init_x, lr=0.01, step_num = 100):
    x = init_x
    
    for i in range(stem_num):
        grad = numerical_gradient(f, x):
        x -= lr * grad
        
    return x
    
# 인수 f는 최적화하려는 함수, init_x는 초기값, lr 는 학습률, stem_num 은 경사법에 따른 반복 횟수를 의미함. 

```

여기서 학습률과 같은 매개변수를 하이퍼 파라미터(hyper parameter)라고 함. 이 매개변수는 사람이 직접 설정해야 하며, 일반적으로는 여러 시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거쳐야 함. 

4.4.2 신경망 학습 알고리즘

학습 절차 4단계

1) 전제 

신경망에는 적응 가능한 편향과 가중치가 있고, 이 매개변수를 조정하는 과정을 학습이라고 함. 

2) 전개 

1단계(미니배치) : 훈련 데이터 중 일부를 무작위로 가져옴. 
2단계(기울기 산출) : 미니배치의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함. 
3단계(매개변수 갱신) : 가중치 매개변수를 기울기 방향으로 조금씩 갱신함. 
4단계(반복) : 1~3단계를 반복함. 

상기와 같이 신경망 학습이 이뤄지는 순서로서 데이터를 무작위로 선정하기 때문에 확률적 경사하강법(Stochastic gradient descent)라고 부름. 









                      
                         






